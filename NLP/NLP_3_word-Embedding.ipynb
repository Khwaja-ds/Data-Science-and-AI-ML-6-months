{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76d761a-ae2b-4546-86aa-426a8daaa8ef",
   "metadata": {},
   "source": [
    "#  Word Embeddings & NLP Algorithms (BoW, TF-IDF, Word2Vec)\n",
    "\n",
    "Natural Language Processing (NLP) requires text to be converted into **numerical representation** so that machine learning algorithms can process it.  \n",
    "In this notebook, we will explore 3 important methods:\n",
    "\n",
    "1. Bag of Words (BoW)  \n",
    "2. TF-IDF (Term Frequency - Inverse Document Frequency)  \n",
    "3. Word2Vec  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002cb4d-236d-4e2f-a3fc-c56d7bc2944e",
   "metadata": {},
   "source": [
    "## 🔹 1. Bag of Words (BoW)\n",
    "\n",
    "\n",
    "- **Idea:** Represent text as a \"bag\" of words, ignoring grammar and word order.  \n",
    "- Each document (sentence/paragraph) is represented by a vector of word counts.  \n",
    "- Vocabulary = all unique words in the dataset.  \n",
    "- Document → vector where each dimension = frequency of a word.  \n",
    "\n",
    "###  Limitations\n",
    "- Ignores word order (so \"dog bites man\" = \"man bites dog\").  \n",
    "- Creates very large and sparse vectors (if vocabulary is huge).  \n",
    "- Doesn't capture meaning, only frequency.  \n",
    "\n",
    "### Example\n",
    "Corpus = [\"I love NLP\", \"I love ML\"]  \n",
    "Vocabulary = {I, love, NLP, ML}  \n",
    "Vectors:  \n",
    "- \"I love NLP\" → [1, 1, 1, 0]  \n",
    "- \"I love ML\"  → [1, 1, 0, 1]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c4e605-95ba-41af-ad8c-0e53fc02508b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>deep</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  deep  enjoy  language  learning  love  machine  natural  nlp  \\\n",
       "0    0     0      0         1         0     1        0        1    0   \n",
       "1    0     0      0         0         1     1        1        0    0   \n",
       "2    1     1      1         0         1     0        0        0    1   \n",
       "\n",
       "   processing  \n",
       "0           1  \n",
       "1           0  \n",
       "2           0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    \"I love natural language processing\",\n",
    "    \"I love machine learning\",\n",
    "    \"I enjoy deep learning and NLP\"\n",
    "]\n",
    "\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430e16d-c5bb-4e54-8cac-f9acfef260f4",
   "metadata": {},
   "source": [
    "## 🔹 2. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "\n",
    "BoW gives all words equal importance, but in real text:\n",
    "- Common words like \"is\", \"the\", \"and\" appear often → not very informative.\n",
    "- Rare words carry **more meaning**.\n",
    "\n",
    "That’s where **TF-IDF** comes in.\n",
    "\n",
    "### Formula\n",
    "- **TF (Term Frequency):**  \n",
    "  How often a word appears in a document.  \n",
    "  \\[\n",
    "  TF(t, d) = \\frac{\\text{Count of term t in document d}}{\\text{Total terms in document d}}\n",
    "  \\]\n",
    "\n",
    "- **IDF (Inverse Document Frequency):**  \n",
    "  How unique a word is across all documents.  \n",
    "  \\[\n",
    "  IDF(t) = \\log \\frac{\\text{Total number of documents}}{1 + \\text{Number of documents containing t}}\n",
    "  \\]\n",
    "\n",
    "- **TF-IDF = TF × IDF**\n",
    "\n",
    "###  Key Point\n",
    "- High for words that are frequent in one document but rare in others.  \n",
    "- Low for words that are common across all documents.  \n",
    "\n",
    "###  Limitations\n",
    "- Still produces sparse vectors (like BoW).  \n",
    "- Ignores semantics and word order.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cee7c10-f2b0-40d8-804a-f8287bc834a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>deep</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.680919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and      deep     enjoy  language  learning      love   machine  \\\n",
       "0  0.000000  0.000000  0.000000  0.528635  0.000000  0.402040  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.517856  0.517856  0.680919   \n",
       "2  0.467351  0.467351  0.467351  0.000000  0.355432  0.000000  0.000000   \n",
       "\n",
       "    natural       nlp  processing  \n",
       "0  0.528635  0.000000    0.528635  \n",
       "1  0.000000  0.000000    0.000000  \n",
       "2  0.000000  0.467351    0.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e044510f-0356-4137-8efb-05f7104abe52",
   "metadata": {},
   "source": [
    "##  3. Word2Vec\n",
    "\n",
    "\n",
    "Unlike BoW & TF-IDF (which are sparse and don't capture meaning), **Word2Vec** uses a shallow neural network to learn **dense word embeddings**.\n",
    "\n",
    "- Each word is represented as a **vector of real numbers** (e.g., 50 or 100 dimensions).\n",
    "- Words with similar meanings have similar vectors.\n",
    "- Famous for capturing semantic relationships:\n",
    "  \\[\n",
    "  \\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\n",
    "  \\]\n",
    "\n",
    "###  Architectures\n",
    "1. **CBOW (Continuous Bag of Words):**\n",
    "   - Predicts the target word from its context (surrounding words).\n",
    "   - Faster, works well with small datasets.\n",
    "   \n",
    "2. **Skip-Gram:**\n",
    "   - Predicts the surrounding words from the target word.\n",
    "   - Better for rare words.\n",
    "\n",
    "###  Advantages\n",
    "- Captures semantic meaning (e.g., \"happy\" and \"joyful\" are close).  \n",
    "- Dense, low-dimensional vectors (efficient).  \n",
    "- Useful for deep learning models.  \n",
    "\n",
    "###  Limitations\n",
    "- Requires large corpus to train well.  \n",
    "- Static embeddings (same vector for a word, regardless of context).  \n",
    "  (Later models like BERT solve this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1d9094-8add-479f-9f0b-ad05db5a2808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'learning':\n",
      " [-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
      "  0.009035   -0.01357392 -0.00709698  0.01879702 -0.00315531  0.00064274\n",
      " -0.00828126 -0.01536538 -0.00301602  0.00493959 -0.00177605  0.01106732\n",
      " -0.00548595  0.00452013  0.01091159  0.01669191 -0.00290748 -0.01841629\n",
      "  0.0087411   0.00114357  0.01488382 -0.00162657 -0.00527683 -0.01750602\n",
      " -0.00171311  0.00565313  0.01080286  0.01410531 -0.01140624  0.00371764\n",
      "  0.01217773 -0.0095961  -0.00621452  0.01359526  0.00326295  0.00037983\n",
      "  0.00694727  0.00043555  0.01923765  0.01012121 -0.01783478 -0.01408312\n",
      "  0.00180291  0.01278507]\n",
      "\n",
      "Most similar words to 'learning':\n",
      "[('and', 0.12486252933740616), ('enjoy', 0.07395090907812119), ('i', 0.04237302392721176), ('deep', 0.01819584146142006), ('love', 0.011071967892348766), ('language', 0.001357130240648985), ('processing', -0.1191045492887497), ('nlp', -0.17424817383289337), ('machine', -0.1754782646894455), ('natural', -0.24708358943462372)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize corpus\n",
    "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Check vector representation\n",
    "print(\"Vector for 'learning':\\n\", w2v_model.wv['learning'])\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nMost similar words to 'learning':\")\n",
    "print(w2v_model.wv.most_similar('learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136952e-e67b-42cd-bbfe-67962b9bdf0c",
   "metadata": {},
   "source": [
    "# 🎯 Final Summary\n",
    "\n",
    "| Method   | Type            | Captures Meaning? | Vector Size | Pros | Cons |\n",
    "|----------|----------------|------------------|-------------|------|------|\n",
    "| BoW      | Frequency-based | ❌ No            | Large & Sparse | Simple, easy | Ignores meaning & order |\n",
    "| TF-IDF   | Weighted counts | ❌ No            | Large & Sparse | Reduces importance of common words | Still ignores meaning |\n",
    "| Word2Vec | Neural Embedding| ✅ Yes           | Dense (50-300) | Captures semantics, efficient | Needs large data, context-independent |\n",
    "\n",
    "✅ Use **BoW/TF-IDF** for simple ML models.  \n",
    "✅ Use **Word2Vec (or modern embeddings like BERT/Glove/FastText)** for deep learning & semantic tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e5f3b-be60-46d8-9aed-34249ba9088d",
   "metadata": {},
   "source": [
    "# ✅ Summary\n",
    "\n",
    "- **BoW**: Simple count of words (ignores order & meaning).  \n",
    "- **TF-IDF**: Weighted version of BoW (gives importance to rare but significant words).  \n",
    "- **Word2Vec**: Learns dense word embeddings that capture semantic meaning of words.  \n",
    "\n",
    "These are fundamental techniques in NLP and are often the first step before applying machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
