{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a9abde",
   "metadata": {},
   "source": [
    "\n",
    "# üìò Natural Language Processing (NLP): Tokenization, Stemming & Lemmatization\n",
    "\n",
    "**What you'll learn:**  \n",
    "- Clear definitions of **NLP** and **NLG**  \n",
    "- The **NLP preprocessing pipeline**  \n",
    "- How to do **Tokenization**, **Stemming**, and **Lemmatization** in NLTK  \n",
    "- When to use each technique (with pros/cons)  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a2ab7",
   "metadata": {},
   "source": [
    "\n",
    "## 1Ô∏è‚É£ Introduction\n",
    "\n",
    "### What is NLP?\n",
    "**Natural Language Processing (NLP)** is a field of AI that helps computers understand, interpret, and generate human language. It powers applications like chatbots, search engines, summarizers, and translators.\n",
    "\n",
    "### What is NLG?\n",
    "**Natural Language Generation (NLG)** is a subfield of NLP focused on generating human-like text from structured or unstructured data. Examples: automated report writing, product descriptions, and conversational responses.\n",
    "\n",
    "### Why text preprocessing matters\n",
    "Raw text is noisy. We typically **normalize and structure** it before modeling. Three core steps you're learning here:\n",
    "- **Tokenization:** Split text into sentences/words.\n",
    "- **Stemming:** Chop words to their rough root forms.\n",
    "- **Lemmatization:** Reduce words to valid dictionary forms (lemmas).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4177b2",
   "metadata": {},
   "source": [
    "\n",
    "## 2Ô∏è‚É£ NLP Pipeline (Overview)\n",
    "\n",
    "```\n",
    "Raw Text\n",
    "   ‚îÇ\n",
    "   ‚îú‚îÄ‚îÄ‚ñ∫ Tokenization (sentences, words)\n",
    "   ‚îÇ         ‚îÇ\n",
    "   ‚îÇ         ‚îú‚îÄ‚îÄ‚ñ∫ Cleaning (lowercase, remove punctuation, keep alphabetic)\n",
    "   ‚îÇ         ‚îî‚îÄ‚îÄ‚ñ∫ Stopword Removal (a, the, is, ...)\n",
    "   ‚îÇ\n",
    "   ‚îú‚îÄ‚îÄ‚ñ∫ Normalization\n",
    "   ‚îÇ         ‚îú‚îÄ‚îÄ‚ñ∫ Stemming (rule-based roots)\n",
    "   ‚îÇ         ‚îî‚îÄ‚îÄ‚ñ∫ Lemmatization (dictionary-based lemmas)\n",
    "   ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚ñ∫ Downstream Tasks (classification, NER, QA, search, etc.)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45aac1",
   "metadata": {},
   "source": [
    "\n",
    "## 3Ô∏è‚É£ Setup (NLTK Resources)\n",
    "\n",
    "Run the cell below **once** to download the required NLTK resources:\n",
    "- `punkt` (tokenizer models)\n",
    "- `stopwords` (list of common English stopwords)\n",
    "- `wordnet` + `omw-1.4` (lemmatizer dictionary data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚¨áÔ∏è One-time downloads (safe to re-run)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea39d8",
   "metadata": {},
   "source": [
    "\n",
    "## 4Ô∏è‚É£ Example Text\n",
    "\n",
    "We'll use the same paragraph across all steps for a fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da38b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paragraph = \"AI, machine learning and deep learning are common terms in enterprise\\nIT and sometimes used interchangeably, especially by companies in their marketing materials.\\nBut there are distinctions. The term AI, coined in the 1950s, refers to the simulation of human\\nintelligence by machines. It covers an ever-changing set of capabilities as new technologies\\nare developed. Technologies that come under the umbrella of AI include machine learning and\\ndeep learning. Machine learning enables software applications to become more accurate at\\npredicting outcomes without being explicitly programmed to do so. Machine learning algorithms\\nuse historical data as input to predict new output values. This approach became vastly more\\neffective with the rise of large data sets to train on. Deep learning, a subset of machine\\nlearning, is based on our understanding of how the brain is structured. Deep learning's\\nuse of artificial neural networks structure is the underpinning of recent advances in AI,\\nincluding self-driving cars and ChatGPT.\\n\"\n",
    "print(paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2121e7",
   "metadata": {},
   "source": [
    "\n",
    "## 5Ô∏è‚É£ Tokenization\n",
    "\n",
    "**Definition:** Splitting text into **sentences** and **words**.  \n",
    "This is often the first step in any NLP pipeline.\n",
    "\n",
    "**Why it matters:** Models and rules operate on tokens, not raw strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(paragraph)\n",
    "\n",
    "print(\"Number of sentences:\", len(sentences))\n",
    "print(\"Sample sentences (first 2):\", sentences[:2])\n",
    "print(\"\\nNumber of word tokens:\", len(words))\n",
    "print(\"Sample word tokens (first 20):\", words[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7020b8a",
   "metadata": {},
   "source": [
    "\n",
    "### Stopwords & Cleaning Helper\n",
    "\n",
    "We'll remove common words like *the, is, and, to,* etc., and keep only alphabetic tokens for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_and_filter(tokens):\n",
    "    cleaned = []\n",
    "    for w in tokens:\n",
    "        w_lower = w.lower()\n",
    "        # keep alphabetic tokens only\n",
    "        if w_lower.isalpha() and w_lower not in stop_words:\n",
    "            cleaned.append(w_lower)\n",
    "    return cleaned\n",
    "\n",
    "cleaned_words = clean_and_filter(words)\n",
    "print(\"Cleaned tokens (first 30):\", cleaned_words[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5e355",
   "metadata": {},
   "source": [
    "\n",
    "## 6Ô∏è‚É£ Stemming (PorterStemmer)\n",
    "\n",
    "**Definition:** Heuristic process that chops endings to reach a **root** (may not be a valid word).  \n",
    "**Pros:** Fast, simple.  \n",
    "**Cons:** Can be **too aggressive** (`studies` ‚Üí `studi`, `better` ‚Üí `better`).\n",
    "\n",
    "> üîß Your original `nlp2.py` kept only stopwords and then stemmed them. Here we fix it to **exclude** stopwords and stem meaningful tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem words sentence-by-sentence\n",
    "stemmed_sentences = []\n",
    "for s in sentences:\n",
    "    tokens = word_tokenize(s)\n",
    "    tokens = clean_and_filter(tokens)  # remove stopwords, keep alphabetic\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    stemmed_sentences.append(\" \".join(stems))\n",
    "\n",
    "print(\"Original sentence (sample):\", sentences[0])\n",
    "print(\"Stemmed sentence (sample): \", stemmed_sentences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba675776",
   "metadata": {},
   "source": [
    "\n",
    "## 7Ô∏è‚É£ Lemmatization (WordNetLemmatizer)\n",
    "\n",
    "**Definition:** Reduces words to their **dictionary/canonical form** using vocabulary + morphology.  \n",
    "**Pros:** More accurate than stemming (`studies` ‚Üí `study`, `mice` ‚Üí `mouse`).  \n",
    "**Cons:** Slightly slower; often needs **POS tags** for best results.\n",
    "\n",
    "> üîß Your original `nlp3.py` had indentation issues. Here it's corrected and aligned with proper stopword removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Simple POS map to WordNet tags\n",
    "from nltk.corpus.reader import wordnet\n",
    "def to_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default\n",
    "\n",
    "lemmatized_sentences = []\n",
    "for s in sentences:\n",
    "    tokens = word_tokenize(s)\n",
    "    tokens = clean_and_filter(tokens)\n",
    "    tagged = pos_tag(tokens)  # [('word','NN'), ...]\n",
    "    lemmas = [lemmatizer.lemmatize(w, to_wordnet_pos(tag)) for w, tag in tagged]\n",
    "    lemmatized_sentences.append(\" \".join(lemmas))\n",
    "\n",
    "print(\"Original sentence (sample):    \", sentences[0])\n",
    "print(\"Lemmatized sentence (sample): \", lemmatized_sentences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45376d0",
   "metadata": {},
   "source": [
    "\n",
    "## 8Ô∏è‚É£ Quick Comparison: Stemming vs Lemmatization\n",
    "\n",
    "Below we compare outputs on the first few sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e43b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(min(3, len(sentences))):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    print(\"Original:     \", sentences[i])\n",
    "    print(\"Stemmed:      \", stemmed_sentences[i])\n",
    "    print(\"Lemmatized:   \", lemmatized_sentences[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d916ad",
   "metadata": {},
   "source": [
    "\n",
    "### üìä Stemming vs Lemmatization (At a Glance)\n",
    "\n",
    "| Feature | Stemming | Lemmatization |\n",
    "|---|---|---|\n",
    "| Output form | Rough root (may not be a word) | Dictionary word (lemma) |\n",
    "| Speed | Faster | Slower |\n",
    "| Accuracy | Lower | Higher |\n",
    "| Needs POS? | No | Recommended |\n",
    "| Typical use | Quick search, lightweight preprocessing | Production NLP, linguistically clean features |\n",
    "\n",
    "**Rule of thumb:**  \n",
    "- Use **stemming** when speed matters and minor errors are acceptable.  \n",
    "- Use **lemmatization** when correctness matters (search quality, analytics, production).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9df7c",
   "metadata": {},
   "source": [
    "\n",
    "## 9Ô∏è‚É£ Real-World Applications\n",
    "\n",
    "- **Search engines:** Normalize queries & documents for better matching.  \n",
    "- **Chatbots/Assistants:** Clean user input before intent classification.  \n",
    "- **Topic Modeling & IR:** Normalize tokens for stable topics/indices.  \n",
    "- **Sentiment Analysis:** Reduce sparsity by merging word variants.  \n",
    "\n",
    "## üîü Summary\n",
    "\n",
    "- **Tokenization** breaks text into usable units.  \n",
    "- **Stopword removal + cleaning** reduces noise.  \n",
    "- **Stemming** is fast but crude; **Lemmatization** is slower but accurate.  \n",
    "- Together, these steps build a strong foundation for downstream NLP tasks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
