{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Natural Language Processing with Hugging Face Transformers\n",
        "\n",
        "This notebook introduces **NLP basics** using the Hugging Face `transformers` library.  \n",
        "We will cover:\n",
        "1. Setting up the environment (GPU, Drive, installing libraries)  \n",
        "2. Tokenization (breaking text into tokens for models)  \n",
        "3. Using a pre-trained language model (GPT-2)  \n",
        "4. Generating text with the model  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "SvyskVo3AxJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 1: Mount Google Drive\n",
        "\n",
        "Google Colab provides temporary storage. To **save files permanently**, we connect Colab to our Google Drive.  \n",
        "This way, any models, datasets, or outputs can be saved directly in Drive.\n"
      ],
      "metadata": {
        "id": "H2PFHLG-A38z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5ecT_PB2fcP",
        "outputId": "bcbf9fc1-30d9-41c2-bbb9-1f4c7dd99a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 2: Check GPU Availability\n",
        "\n",
        "Deep learning models like GPT-2 are **heavy**.  \n",
        "- Running them on CPU is very slow.  \n",
        "- GPUs speed up training and inference drastically.  \n",
        "\n",
        "Here we use `nvidia-smi` to check GPU info (type, memory, usage).\n"
      ],
      "metadata": {
        "id": "2KjOV5ZbBCcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlkesk8h23Tp",
        "outputId": "ace47511-3de7-4d51-c087-fe79282e2a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 18 11:08:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 3: Install Hugging Face Transformers\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) provides pre-trained NLP models (BERT, GPT, RoBERTa, etc).  \n",
        "We install the `transformers` library which allows us to:\n",
        "- Load pre-trained models  \n",
        "- Perform tokenization  \n",
        "- Do text classification, translation, summarization, etc.\n"
      ],
      "metadata": {
        "id": "gxeDEaeIBJZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK8Fcqnh3eoT",
        "outputId": "5f0c98bd-c7e0-4605-cc3f-d922b0c3b3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 4: Tokenization\n",
        "\n",
        "**What is Tokenization?**  \n",
        "- Process of converting raw text into tokens (numbers) that models understand.  \n",
        "- Example: `\"Hello, how are you?\"` â†’ `[15496, 11, 4919, 389, 345]`  \n",
        "\n",
        "Here we use the GPT-2 tokenizer:\n",
        "- `AutoTokenizer.from_pretrained(\"gpt2\")` loads GPT-2 tokenizer.  \n",
        "- `return_tensors=\"pt\"` converts tokens into PyTorch tensors.\n"
      ],
      "metadata": {
        "id": "RHZEckJfBOIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "text = \"Hello,how are you\"\n",
        "tokens = tokenizer(text,return_tensors=\"pt\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOP3PZEG37j2",
        "outputId": "73942f1c-1ff0-4718-9ae2-b6de92c23bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[15496,    11,  4919,   389,   345]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 5: Load Pre-trained GPT-2 Model\n",
        "\n",
        "We now load the **GPT-2 model**:  \n",
        "- `AutoModelForCausalLM` loads GPT-2 for **text generation**.  \n",
        "- We encode an input prompt (e.g., `\"Mughals\"`) into tokens.  \n",
        "- `model.generate()` produces text continuation.  \n",
        "- Finally, we decode tokens back into human-readable text.\n"
      ],
      "metadata": {
        "id": "a5eTe5O9BSma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "input_ids = tokenizer.encode(\"Mughals\",return_tensors=\"pt\")\n",
        "output = model.generate(input_ids,max_length=50)\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-YQ_X2b45i9",
        "outputId": "961a4017-ff9a-43c6-eaff-45ca46f5bc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mughals, who were the first to establish a foothold in the Indian Ocean, were the first to establish a foothold in the Indian Ocean, and the first to establish a foothold in the Indian Ocean.\n",
            "\n",
            "The first Indian Ocean expedition was launched\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Summary\n",
        "\n",
        "- **Google Drive + GPU** setup ensures smooth work on large models.  \n",
        "- **Tokenization** converts raw text into tokens.  \n",
        "- **GPT-2 model** can generate text given a prompt.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "slSYWM3mBX36"
      }
    }
  ]
}